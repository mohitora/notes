3 Nodes
[ALL] -> Action to be performed on all nodes

 rhc101 - Node1
 rhc102 - Node2
 rhc103 - Node3

- [ALL] Make sure firewall is disabled. (iptables -L)
- [ALL] Make sure selinux is disabled.(sestatus)
- [ALL] Update the hosts entry ( /etc/hosts)
- [ALL] Install below packages ( enable repo - rhel-ha-for-rhel-7-server-rpms)
yum install pcs pacemaker fence-agents lvm2-cluster resource-agents psmisc policycoreutils-python gfs2-utils --enablerepo=rhel-ha-for-rhel-7-server-rpms

- [ALL] Create user hacluster and set password for the user
- [ALL] Make sure password never expire.
[root@rhc101 ~]# chage -l hacluster
Last password change					: Jan 25, 2019
Password expires					: never
Password inactive					: never
Account expires						: never
Minimum number of days between password change		: -1
Maximum number of days between password change		: -1
Number of days of warning before password expires	: -1
[root@rhc101 ~]#


- [All] Start the PCS service
[root@rhc101 ~]# systemctl start pcsd
[root@rhc101 ~]# systemctl status pcsd
● pcsd.service - PCS GUI and remote configuration interface
   Loaded: loaded (/usr/lib/systemd/system/pcsd.service; disabled; vendor preset: disabled)
   Active: active (running) since Fri 2019-01-25 03:10:33 CST; 7s ago
     Docs: man:pcsd(8)
           man:pcs(8)
 Main PID: 17991 (pcsd)
   CGroup: /system.slice/pcsd.service
           └─17991 /usr/bin/ruby /usr/lib/pcsd/pcsd

Jan 25 03:10:32 rhc101.ihah.ibm systemd[1]: Starting PCS GUI and remote configuration interface...
Jan 25 03:10:33 rhc101.ihah.ibm systemd[1]: Started PCS GUI and remote configuration interface.
[root@rhc101 ~]#

- [RHC101] In browser check the UI -> https://rhc101:2224/login
(user hacluster/password)

- [ALL] Enable pcsd service
[root@rhc101 ~]# systemctl enable pcsd
Created symlink from /etc/systemd/system/multi-user.target.wants/pcsd.service to /usr/lib/systemd/system/pcsd.service.
[root@rhc101 ~]#

- [ALL] Authrize the nodes
[root@rhc101 ~]# pcs cluster auth rhc101 rhc102 rhc103 -u hacluster
Password:
rhc101: Authorized
rhc102: Authorized
rhc103: Authorized
[root@rhc101 ~]#

- [RHC101] Pcs cluster setup
[root@rhc101 ~]# pcs cluster setup --start --name TestCluster rhc101 rhc102 rhc103
Destroying cluster on nodes: rhc101, rhc102, rhc103...
rhc102: Stopping Cluster (pacemaker)...
rhc101: Stopping Cluster (pacemaker)...
rhc103: Stopping Cluster (pacemaker)...
rhc101: Successfully destroyed cluster
rhc103: Successfully destroyed cluster
rhc102: Successfully destroyed cluster

Sending 'pacemaker_remote authkey' to 'rhc101', 'rhc102', 'rhc103'
rhc101: successful distribution of the file 'pacemaker_remote authkey'
rhc103: successful distribution of the file 'pacemaker_remote authkey'
rhc102: successful distribution of the file 'pacemaker_remote authkey'
Sending cluster config files to the nodes...
rhc101: Succeeded
rhc102: Succeeded
rhc103: Succeeded

Starting cluster on nodes: rhc101, rhc102, rhc103...
rhc101: Starting Cluster (corosync)...
rhc102: Starting Cluster (corosync)...
rhc103: Starting Cluster (corosync)...
rhc103: Starting Cluster (pacemaker)...
rhc101: Starting Cluster (pacemaker)...
rhc102: Starting Cluster (pacemaker)...

Synchronizing pcsd certificates on nodes rhc101, rhc102, rhc103...
rhc101: Success
rhc102: Success
rhc103: Success
Restarting pcsd on the nodes in order to reload the certificates...
rhc101: Success
rhc102: Success
rhc103: Success
[root@rhc101 ~]#



- [ALL] Check the pcs status
[root@rhc102 yum.repos.d]# pcs cluster status
Cluster Status:
 Stack: corosync
 Current DC: rhc102 (version 1.1.19-8.el7_6.2-c3c624ea3d) - partition with quorum
 Last updated: Fri Jan 25 03:59:20 2019
 Last change: Fri Jan 25 03:58:03 2019 by hacluster via crmd on rhc102
 3 nodes configured
 0 resources configured

PCSD Status:
  rhc102: Online
  rhc103: Online
  rhc101: Online
[root@rhc102 yum.repos.d]#

- [RHC101] Start and Enable pcs for all from one node.
[root@rhc101 ~]# pcs cluster start --all
rhc101: Starting Cluster (corosync)...
rhc102: Starting Cluster (corosync)...
rhc103: Starting Cluster (corosync)...
rhc102: Starting Cluster (pacemaker)...
rhc101: Starting Cluster (pacemaker)...
rhc103: Starting Cluster (pacemaker)...
[root@rhc101 ~]# pcs cluster enable --all
rhc101: Cluster Enabled
rhc102: Cluster Enabled
rhc103: Cluster Enabled
[root@rhc101 ~]#


- [ALL] check status
[root@rhc101 ~]# crm_mon -r1
Stack: corosync
Current DC: rhc103 (version 1.1.19-8.el7_6.2-c3c624ea3d) - partition with quorum
Last updated: Fri Jan 25 04:17:06 2019
Last change: Fri Jan 25 03:58:03 2019 by hacluster via crmd on rhc102

3 nodes configured
0 resources configured

Online: [ rhc101 rhc102 rhc103 ]

No resources

[root@rhc101 ~]#

- [RHC01] crm_verify -L -V
 error: unpack_resources: Resource start-up disabled since no STONITH resources have been defined
 error: unpack_resources: Either configure some or disable STONITH with the stonith-enabled option
 error: unpack_resources: NOTE: Clusters with shared data need STONITH to ensure data integrity
 Errors found during check: config not valid


- [RHC101] Set properties

[root@rhc101 ~]# pcs property set stonith-enabled=false
[root@rhc101 ~]# pcs property set no-quorum-policy=stop;
[root@rhc101 ~]# pcs property
Cluster Properties:
 cluster-infrastructure: corosync
 cluster-name: TestCluster
 dc-version: 1.1.19-8.el7_6.2-c3c624ea3d
 have-watchdog: false
 no-quorum-policy: stop
 stonith-enabled: false


- [RHC101] Virtual IP check
[root@rhc101 ~]# ping 10.87.102.3
PING 10.87.102.3 (10.87.102.3) 56(84) bytes of data.
^C
--- 10.87.102.3 ping statistics ---
14 packets transmitted, 0 received, 100% packet loss, time 12999ms

[root@rhc101 ~]# nic=`ifconfig -a | grep -B1 'inet 10.' | head -1 | cut -d ':' -f1 |sed 's/^/####/g' | grep "^####" | cut -d "#" -f5`
[root@rhc101 ~]# echo $nic
eth0
[root@rhc101 ~]# pcs resource create HAProxyVIP ocf:heartbeat:IPaddr2 ip=10.87.102.3 cidr_netmask=26 nic=${nic}:1 op monitor interval=30s --group haproxygrp
[root@rhc101 ~]# ping -c1 10.87.102.3
PING 10.87.102.3 (10.87.102.3) 56(84) bytes of data.
64 bytes from 10.87.102.3: icmp_seq=1 ttl=64 time=0.025 ms

--- 10.87.102.3 ping statistics ---

- [RHC101] Check the status of VIP and see which node is VIP belongs to.
[root@rhc101 ~]# pcs status
Cluster name: TestCluster
Stack: corosync
Current DC: rhc103 (version 1.1.19-8.el7_6.2-c3c624ea3d) - partition with quorum
Last updated: Sat Jan 26 09:07:45 2019
Last change: Sat Jan 26 08:52:24 2019 by root via cibadmin on rhc101

3 nodes configured
1 resource configured

Online: [ rhc101 rhc102 rhc103 ]

Full list of resources:

 Resource Group: haproxygrp
     HAProxyVIP	(ocf::heartbeat:IPaddr2):	Started rhc101

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled
[root@rhc101 ~]#

[ALL] Installing apache on all nodes
yum -y install httpd

- [ALL] cat /etc/httpd/conf.d/serverstatus.conf
Listen 127.0.0.1:80
 <Location /server-status>
 SetHandler server-status
 Order deny,allow
 Deny from all
 Allow from 127.0.0.1
</Location>

- [ALL] sudo sed -i 's/Listen/#Listen/' /etc/httpd/conf/httpd.conf

Start Apache on all nodes  and verify if the status page is working

sudo systemctl restart httpd
wget http://127.0.0.1/server-status

- [ALL] Index file
cat /var/www/html/index.html
 <html>
 <h1>rhc101</h1>
 </html>

- [ALL] First stop apache
echo "Listen 10.87.102.3:80"|sudo tee --append /etc/httpd/conf/httpd.conf

- [ALL] Add a resource 
pcs resource create webserver ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl="http://localhost/server-status" op monitor interval=1min

[RHC101] By default, the cluster will try to balance the resources over the cluster. That means that the virtual IP, which is a resource, will be started on a different node than the webserver-resource. Starting the webserver on a node that isn’t the owner of the virtual IP will cause it to fail since we configured Apache to listen on the virtual IP. In order to make sure that the virtual IP and webserver always stay together, we can add a constraint

sudo pcs constraint colocation add webserver HAProxyVIP INFINITY

- [RHC101] To avoid the situation where the webserver would start before the virtual IP is started or owned by a certain node, we need to add another constraint which determines the order of availability of both resources
pcs constraint order  HAProxyVIP then webserver

- [RHC101] When both the cluster nodes are not equally powered machines and you would like the resources to be available on the most powerful machine, you can add another constraint for location
pcs constraint location webserver prefers rhc101=50

- [RHC101] Check the constraint and start and stop the pcs cluster
pcs constraint
pcs cluster stop --all && sudo pcs cluster start --all




Ref URL :
http://jensd.be/156/linux/building-a-high-available-failover-cluster-with-pacemaker-corosync-pcs 
